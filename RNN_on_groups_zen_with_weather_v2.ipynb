{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZenanAH/Team_Energy/blob/master/RNN_on_groups_zen_with_weather_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e706e0fd",
      "metadata": {
        "id": "e706e0fd"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras .models import Sequential\n",
        "from tensorflow.keras .layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
        "from tensorflow.keras .optimizers import SGD\n",
        "import math\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8f838c8d",
      "metadata": {
        "id": "8f838c8d"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow\n",
        "name=\"Q\"\n",
        "tariff='Std'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6fb57830"
      },
      "outputs": [],
      "source": [
        "def split_data(filename,tariff):\n",
        "#     print('Input precise path for data including extension')\n",
        "#     filename = input()\n",
        "    fulldata = pd.read_csv(filename)\n",
        "    fulldata['DateTime'] = pd.to_datetime(fulldata['DateTime'])\n",
        "    \n",
        "    if tariff==\"ToU\":\n",
        "      start_date='2013-01-01'\n",
        "    else: \n",
        "      start_date='2012-01-01'\n",
        "\n",
        "    train_data = fulldata[(fulldata['DateTime'] >= start_date) & (fulldata['DateTime'] < '2014-01-01')].reset_index(drop = True)\n",
        "    validation_data = fulldata[(fulldata['DateTime'] >= '2014-01-01') & (fulldata['DateTime'] < '2014-02-01')].reset_index(drop = True)\n",
        "    test_data = fulldata[(fulldata['DateTime'] >= '2014-01-01') & (fulldata['DateTime'] < '2014-03-01')].reset_index(drop = True)\n",
        "    return train_data, validation_data, test_data\n",
        "\n",
        "def create_data(name,tariff):\n",
        "    tdata, vdata, testd=split_data(f'https://storage.googleapis.com/energy_usage_prediction_903/df_{name}_avg_{tariff}_v1.csv',tariff)\n",
        "    # add val and train for prophet\n",
        "    combine_tr_vl=False\n",
        "    \n",
        "    if combine_tr_vl==True:\n",
        "      pdata=pd.concat([tdata,vdata],axis=0).reset_index(drop=True)\n",
        "    else:\n",
        "      pdata=tdata\n",
        "\n",
        "    global_average=False\n",
        "\n",
        "    if global_average==False:\n",
        "        # not for global average\n",
        "        tdata.drop(columns='Unnamed: 0',inplace=True)\n",
        "        testd.drop(columns='Unnamed: 0',inplace=True)\n",
        "        vdata.drop(columns='Unnamed: 0',inplace=True)\n",
        "        \n",
        "    # group by\n",
        "    df5=pdata.loc[:,['DateTime','KWH/hh']]\n",
        "    df5.set_index('DateTime',inplace=True)\n",
        "\n",
        "    train_df=df5.groupby(by=df5.index).mean()\n",
        "    train_df=train_df.reset_index()\n",
        "\n",
        "    test_df=testd.loc[:,['DateTime','KWH/hh']].groupby(by=testd['DateTime']).mean()\n",
        "    test_df.reset_index(inplace=True)\n",
        "\n",
        "    val_df=vdata.loc[:,['DateTime','KWH/hh']].groupby(by=vdata['DateTime']).mean()\n",
        "    val_df.reset_index(inplace=True)\n",
        "    \n",
        "    return train_df,test_df,val_df\n",
        "\n",
        "def get_weather(train_df, test_df,val_df,tariff):\n",
        "    twd, vwd, testwd=split_data('https://storage.googleapis.com/weather-data-processed-for-le-wagon/cleaned_weather_hourly_darksky.csv',tariff)\n",
        "  \n",
        "    combine_tr_vl=False\n",
        "    \n",
        "    if combine_tr_vl==True:\n",
        "      wd=pd.concat([twd,vwd],axis=0).reset_index(drop=True)\n",
        "    else:\n",
        "      wd=twd\n",
        "\n",
        "    \n",
        "    wd_filt=wd[['DateTime','temperature','windSpeed','precipType_rain']].dropna()\n",
        "    wd_filt['DateTime']=pd.to_datetime(wd_filt['DateTime'])\n",
        "    wd_filt[wd_filt.columns[1:].to_list()]=wd_filt[wd_filt.columns[1:].to_list()].interpolate(method='linear')\n",
        "    \n",
        "    test_wd=testwd[['DateTime','temperature','windSpeed','precipType_rain']].dropna()\n",
        "    test_wd['DateTime']=pd.to_datetime(test_wd['DateTime'])\n",
        "    \n",
        "    val_wd=vwd[['DateTime','temperature','windSpeed','precipType_rain']].dropna()\n",
        "    val_wd['DateTime']=pd.to_datetime(val_wd['DateTime'])\n",
        "    # # wind = wd_filt['windSpeed'].interpolate(method='linear')\n",
        "    # # rain = wd_filt['precipType_rain'].interpolate(method='linear')  \n",
        "    train_wd=train_df[['DateTime']].merge(wd_filt,on='DateTime',how='inner')\n",
        "    test_wd=test_df[['DateTime']].merge(test_wd,on='DateTime',how='inner')\n",
        "    val_wd=val_df[['DateTime']].merge(val_wd,on='DateTime',how='inner')\n",
        "\n",
        "    train_wd[train_wd.columns[1:].to_list()]=train_wd[train_wd.columns[1:].to_list()].interpolate(method='linear')\n",
        "    test_wd[test_wd.columns[1:].to_list()]=test_wd[test_wd.columns[1:].to_list()].interpolate(method='linear')\n",
        "    val_wd[val_wd.columns[1:].to_list()]=val_wd[test_wd.columns[1:].to_list()].interpolate(method='linear')\n",
        "\n",
        "    return train_wd, test_wd,val_wd\n",
        "\n",
        "def plot_predictions(df_plot):\n",
        "    plt.figure(figsize=(18,6))\n",
        "    sns.lineplot(x=df_plot['DateTime'],y=df_plot['Predicted'],label='Forecast')\n",
        "    sns.lineplot(x=df_plot['DateTime'],y=df_plot['y'],label='Actual')\n",
        "    plt.title('Electricity Consumption Prediction')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Consumption (kWh/hh)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def return_mape(test,predicted):\n",
        "    mape = mean_absolute_percentage_error(test, predicted)\n",
        "    print(\"The  mean absolute percenatge error is {}.\".format(mape))"
      ],
      "id": "6fb57830"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67b1ffc8",
      "metadata": {
        "id": "67b1ffc8"
      },
      "outputs": [],
      "source": [
        "#join weather and consumption\n",
        "train_df,test_df,val_df=create_data(name,tariff)\n",
        "train_wd, test_wd,val_wd=get_weather(train_df, test_df,val_df,tariff)\n",
        "train=train_wd.merge(train_df, on='DateTime')\n",
        "test=test_wd.merge(test_df, on='DateTime')\n",
        "val=val_wd.merge(val_df, on='DateTime')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4d8861ea",
      "metadata": {
        "id": "4d8861ea"
      },
      "outputs": [],
      "source": [
        "def prepare_sequences(train_df,test_df,val_df):\n",
        "  train_df.set_index('DateTime',inplace=True)\n",
        "  test_df.set_index('DateTime',inplace=True)\n",
        "  val_df.set_index('DateTime',inplace=True)\n",
        "  training_set = train_df.loc[:,:].values\n",
        "  test_set = test_df.loc[:,:].values\n",
        "  # Scaling the training set\n",
        "  sc = MinMaxScaler(feature_range=(0,1))\n",
        "  training_set_scaled = sc.fit_transform(training_set)\n",
        "  #Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n",
        "  # So for each element of training set, we have 60 previous training set elements \n",
        "  X_train = []\n",
        "  y_train = []\n",
        "  for i in range(60,len(training_set_scaled)):\n",
        "      X_train.append(training_set_scaled[i-60:i,0])\n",
        "      y_train.append(training_set_scaled[i,0])\n",
        "  X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "  # Reshaping X_train for efficient modelling\n",
        "  X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
        "  # Now to get the test set ready in a similar way as the training set.\n",
        "  # The following has been done so first 60 entries of test set have 60 previous values which is impossible to get \n",
        "  dataset_total = pd.concat((train_df[\"KWH/hh\"][:'2013'],test_df[\"KWH/hh\"]['2014':]),axis=0)\n",
        "  inputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values\n",
        "  inputs = inputs.reshape(-1,1)\n",
        "  inputs  = sc.transform(inputs)\n",
        "  # Preparing X_test and predicting the prices\n",
        "  X_test = []\n",
        "  for i in range(60,len(inputs)):\n",
        "      X_test.append(inputs[i-60:i,0])\n",
        "  X_test = np.array(X_test)\n",
        "  X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
        "\n",
        "  return  X_train,y_train,X_test,sc,test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "426997d7",
      "metadata": {
        "id": "426997d7"
      },
      "outputs": [],
      "source": [
        "X_train,y_train,X_test,sc,test_set=prepare_sequences(train_df,test_df,val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f7603f",
      "metadata": {
        "id": "e7f7603f",
        "outputId": "bdcb9d76-5e2c-4f5f-c820-1bbe9973ff74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1095/1095 [==============================] - 173s 145ms/step - loss: 3536.0300\n",
            "Epoch 2/50\n",
            "1095/1095 [==============================] - 157s 144ms/step - loss: 667.3783\n",
            "Epoch 3/50\n",
            "1095/1095 [==============================] - 155s 141ms/step - loss: 905.2248\n",
            "Epoch 4/50\n",
            "1095/1095 [==============================] - 153s 139ms/step - loss: 1272.7743\n",
            "Epoch 5/50\n",
            " 866/1095 [======================>.......] - ETA: 34s - loss: 733.1254"
          ]
        }
      ],
      "source": [
        "# The LSTM architecture\n",
        "regressor = Sequential()\n",
        "# First LSTM layer with Dropout regularisation\n",
        "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
        "regressor.add(Dropout(0.2))\n",
        "# Second LSTM layer\n",
        "regressor.add(LSTM(units=50, return_sequences=True))\n",
        "regressor.add(Dropout(0.2))\n",
        "# Third LSTM layer\n",
        "regressor.add(LSTM(units=50, return_sequences=True))\n",
        "regressor.add(Dropout(0.2))\n",
        "# Fourth LSTM layer\n",
        "regressor.add(LSTM(units=50))\n",
        "regressor.add(Dropout(0.2))\n",
        "# The output layer\n",
        "regressor.add(Dense(units=1))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressor.compile(optimizer='rmsprop',loss='MeanAbsolutePercentageError')\n",
        "# Fitting to the training set\n",
        "regressor.fit(X_train,y_train,epochs=50,batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d026da1",
      "metadata": {
        "id": "3d026da1"
      },
      "outputs": [],
      "source": [
        "#X_train,y_train,X_test,sc,test_set=prepare_sequences(train_df,test_df,val_df)\n",
        "# prediction\n",
        "predicted_consumption = regressor.predict(X_test)\n",
        "predicted_consumption = sc.inverse_transform(predicted_consumption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2443d7",
      "metadata": {
        "id": "cf2443d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualizing the results for LSTM\n",
        "df_plot=test_df\n",
        "df_plot=df_plot.merge(val_df,left_on=df_plot.index,right_on=val_df.index,how='outer').fillna('')\n",
        "df_plot.drop(columns='KWH/hh_x',inplace=True)\n",
        "df_plot.rename(columns={'key_0':'DateTime','KWH/hh_y':'y'},inplace=True)\n",
        "df_plot['Predicted']=predicted_consumption\n",
        "plot_predictions(df_plot)\n",
        "# Evaluating our model\n",
        "return_mape(test_set,predicted_consumption)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def metric(predicted_consumption):\n",
        "  #calculations\n",
        "  #ofgem 2018 figures\n",
        "  elecf=[1.196367,1.164487,1.052873,0.949678,0.910947,0.863625,0.846739,0.857709,0.877138,0.962776,1.095168,1.222492]\n",
        "  #BEIS 2018 figures\n",
        "  co2=0.309 # kg/kwh emission\n",
        "  annual_consumption=round((predicted_consumption.sum()/(elecf[0]+elecf[1]))*np.sum(elecf),2)\n",
        "  annual_co2=round(co2*annual_consumption,2)\n",
        "  #if no renewable energy contribution either from supplier or from self generation\n",
        "  return annual_consumption,annual_co2\n",
        "\n",
        "annual_consumption,annual_co2=metric(predicted_consumption)\n",
        "print(f'You total predicted annual_consumption in {annual_consumption} kWh')\n",
        "print(f'Your total predicted annual carbon footprint amounts to {annual_co2} kg')"
      ],
      "metadata": {
        "id": "vkizk9uVuGQx"
      },
      "id": "vkizk9uVuGQx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(regressor,f'model_{name}_{tariff}.joblib')"
      ],
      "metadata": {
        "id": "jaLhFNwAigKn"
      },
      "id": "jaLhFNwAigKn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}